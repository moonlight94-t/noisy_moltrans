{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1734524820418,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "U9aQ_JEMBQjA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir('/content/drive/MyDrive/mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3471,
     "status": "ok",
     "timestamp": 1734524824358,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "l1PBvnN3BbQG",
    "outputId": "5278e3cd-ce78-4f0a-b38f-975a09910d74",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: subword-nmt in /opt/conda/lib/python3.10/site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.10/site-packages (from subword-nmt) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from subword-nmt) (4.67.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17214,
     "status": "ok",
     "timestamp": 1734524841568,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "N0Gg5-8UkbNV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import time\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, \\\n",
    "    precision_score, recall_score, auc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import collections\n",
    "import math\n",
    "import copy\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import codecs\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(3)\n",
    "\n",
    "from model import BIN_Interaction_Flat1, BIN_Interaction_Flat2, BIN_Interaction_Flat3\n",
    "from stream import BIN_Data_Encoder\n",
    "from data import Cus_DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734524841568,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "NDO4E5H6kDzY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(data_generator, model):\n",
    "  torch.cuda.empty_cache()\n",
    "  y_pred = []\n",
    "  y_label = []\n",
    "  model.eval()\n",
    "  loss_accumulate = 0.0\n",
    "  count = 0.0\n",
    "  for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n",
    "    score = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda()) # long tensor int64, .to(device)를 왜 사용안했지?\n",
    "\n",
    "    m = torch.nn.Sigmoid()\n",
    "    logits = torch.squeeze(m(score)) # 차원이 1인 축 제거\n",
    "    loss_fct = torch.nn.BCELoss()\n",
    "\n",
    "    label = label.float().cuda()\n",
    "\n",
    "    loss = loss_fct(logits, label)\n",
    "\n",
    "    loss_accumulate += loss.item()\n",
    "    count += 1\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    label_ids = label.detach().to('cpu').numpy()\n",
    "    y_label = y_label + label_ids.flatten().tolist()\n",
    "    y_pred = y_pred + logits.flatten().tolist()\n",
    "\n",
    "  loss = loss_accumulate / count\n",
    "\n",
    "  fpr, tpr, thresholds = roc_curve(y_label, y_pred) # TPR: 양성을 잘 잡아내는 능력 값이 클수록 재현율이 높음, FPR: 음성을 얼마나 잘 음성으로 유지하는지 값이 작을수록 좋으며 음성을 양성으로 잘못 분류하는 비율이 낮음을 의미\n",
    "\n",
    "  precision = tpr / (tpr + fpr+0.00001)\n",
    "\n",
    "  f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n",
    "\n",
    "  thred_optim = thresholds[2:][np.argmax(f1[2:])] # threshold는 y_pred에 대해 내림차순으로 뱉어내므로 너무 strict한 threshold 5개는 고려하지않음 그 이유는?\n",
    "\n",
    "  print(\"optimal threshold: \" + str(thred_optim))\n",
    "\n",
    "  y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n",
    "\n",
    "  auc_k = auc(fpr, tpr)\n",
    "  print(\"AUROC:\" + str(auc_k))\n",
    "  print(\"AUPRC: \" + str(average_precision_score(y_label, y_pred))) # 여러 threshold 기준 recall-precision auc\n",
    "\n",
    "  cm1 = confusion_matrix(y_label, y_pred_s)\n",
    "  print('Confusion Matrix : \\n', cm1)\n",
    "  print('Recall : ', recall_score(y_label, y_pred_s))\n",
    "  print('Precision : ', precision_score(y_label, y_pred_s)) # 단일 threshold 기준(최적값)\n",
    "\n",
    "  total1 = sum(sum(cm1))\n",
    "  #####from confusion matrix calculate accuracy\n",
    "  accuracy1 = (cm1[0, 0] + cm1[1, 1]) / total1\n",
    "  print('Accuracy : ', accuracy1)\n",
    "\n",
    "  sensitivity1 = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n",
    "  print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "  specificity1 = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n",
    "  print('Specificity : ', specificity1)\n",
    "\n",
    "  outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n",
    "  return roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label, outputs), y_pred, loss\n",
    "\n",
    "def train(model,training_generator,epochs,batchsize,lr):\n",
    "  torch.cuda.empty_cache()\n",
    "  loss_history = []\n",
    "  #model = model.cuda()\n",
    "\n",
    "  opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "  print('--- Data Preparation ---')\n",
    "  params = {'batch_size': batchsize, 'shuffle': True, 'drop_last': True}\n",
    "\n",
    "  dataFolder = os.getcwd()\n",
    "  df_val = pd.read_csv(dataFolder + '/val.csv') #valid_new / test_new\n",
    "  df_test = pd.read_csv(dataFolder + '/test.csv')\n",
    "\n",
    "  validation_set = BIN_Data_Encoder(df_val.index.values, df_val.Label.values, df_val)\n",
    "  validation_generator = DataLoader(validation_set, **params)\n",
    "\n",
    "  testing_set = BIN_Data_Encoder(df_test.index.values, df_test.Label.values, df_test)\n",
    "  testing_generator = DataLoader(testing_set, **params)\n",
    "\n",
    "  # early stopping\n",
    "  max_auc = 0\n",
    "  model_max = copy.deepcopy(model.to('cpu'))\n",
    "  epo_count = 0\n",
    "\n",
    "  with torch.no_grad(): # with torch.no_grad():\n",
    "    model_max = model_max.cuda()\n",
    "    auc, auprc, f1, logits, loss = test(testing_generator, model_max)\n",
    "    print('Initial Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: ' + str(f1) + ' , Test loss: ' + str(loss))\n",
    "    model_max = model_max.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  model=model.cuda()\n",
    "  print('--- Go for Training ---')\n",
    "  for epo in range(epochs):\n",
    "    model.train()\n",
    "    count=0.0\n",
    "    loss_accumulate = 0.0\n",
    "    for i,(d, p, d_mask, p_mask, label) in tqdm(enumerate(training_generator),desc=\"training\"):\n",
    "      score = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n",
    "\n",
    "      label = label.float().cuda()\n",
    "\n",
    "      loss_fct = torch.nn.BCELoss()\n",
    "      m = torch.nn.Sigmoid()\n",
    "      n = torch.squeeze(m(score))\n",
    "\n",
    "      loss = loss_fct(n, label)\n",
    "\n",
    "      loss_accumulate += loss.cpu().detach().numpy().item()\n",
    "      count += 1\n",
    "\n",
    "      opt.zero_grad()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "    loss = loss_accumulate / count\n",
    "    loss_history.append(loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    if ((epo+1) % 10 == 0):\n",
    "      print('Training at Epoch ' + str(epo + 1) + ' with loss ' + str(loss))\n",
    "\n",
    "    # every epoch test\n",
    "    with torch.set_grad_enabled(False):\n",
    "      auc, auprc, f1, logits, loss = test(validation_generator, model)\n",
    "      if auc > max_auc:\n",
    "        model_max = copy.deepcopy(model.to('cpu'))\n",
    "        model=model.cuda()\n",
    "        epo_count = 0\n",
    "        max_auc = auc\n",
    "        print('--------'+str(epo+1)+'model_max updated-------')\n",
    "      else:\n",
    "        epo_count += 1\n",
    "      print('Validation at Epoch ' + str(epo + 1) + ' , AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: ' + str(f1) + ' , Validation loss: ' + str(loss))\n",
    "      torch.cuda.empty_cache()\n",
    "      if epo_count == 20 :\n",
    "        break\n",
    "  model.cpu()\n",
    "  del model\n",
    "  torch.cuda.empty_cache()\n",
    "  print('--- Go for Testing ---')\n",
    "  try:\n",
    "    with torch.set_grad_enabled(False):\n",
    "      model_max=model_max.cuda()\n",
    "      auc, auprc, f1, logits, loss = test(testing_generator, model_max)\n",
    "      print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: ' + str(f1) + ' , Test loss: ' + str(loss))\n",
    "      model_max=model_max.cpu()\n",
    "      torch.cuda.empty_cache()\n",
    "  except:\n",
    "      print('testing failed')\n",
    "  return model_max, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734524841568,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "XqXq-Y6b5Tl8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BIN_config_DBPE():\n",
    "    config = {}\n",
    "    config['batch_size'] = [4,32,64]\n",
    "    config['input_dim_drug'] = 23532\n",
    "    config['input_dim_target'] = 16693\n",
    "    config['max_drug_seq'] = 50 #64\n",
    "    config['max_protein_seq'] = 545 #576\n",
    "    config['emb_size'] = 384\n",
    "    config['dropout_rate'] = [0.1,0.15,0.2]\n",
    "\n",
    "    #DenseNet\n",
    "    # config['scale_down_ratio'] = 0.25\n",
    "    # config['growth_rate'] = 20\n",
    "    # config['transition_rate'] = 0.5\n",
    "    # config['num_dense_blocks'] = 4\n",
    "    config['kernal_dense_size'] = 3\n",
    "\n",
    "    # Encoder\n",
    "    config['intermediate_size'] = 1536\n",
    "    config['num_attention_heads'] = 12\n",
    "    config['attention_probs_dropout_prob'] = [0.1,0.15,0.2]\n",
    "    config['hidden_dropout_prob'] = [0.1,0.15,0.2]\n",
    "    config['flat_dim'] = 78192 #106764\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734524841568,
     "user": {
      "displayName": "최현영",
      "userId": "12280114103183972011"
     },
     "user_tz": -540
    },
    "id": "tWaLIOo7uuj6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  batchsize = 4 # 16\n",
    "  epochs =350 # 50\n",
    "  #epochs_fine=50\n",
    "  lr = 5e-5 #5e-5\n",
    "  confidence=0.55\n",
    "  ratio1=7\n",
    "  ratio2=15\n",
    "  config = BIN_config_DBPE()\n",
    "  model = BIN_Interaction_Flat1(**config)\n",
    "  model2 = BIN_Interaction_Flat2(**config)\n",
    "  model3 = BIN_Interaction_Flat3(**config)\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "  dataFolder = os.getcwd()\n",
    "  df_train = pd.read_csv(dataFolder + '/train.csv') #train_new\n",
    "  df_unlabel = pd.read_csv(dataFolder + '/result.csv')\n",
    "\n",
    "  params = {'batch_size': batchsize, 'shuffle': True, 'drop_last': True}\n",
    "  params2 = {'batch_size': batchsize*16, 'shuffle': True, 'drop_last': True}\n",
    "  training_set = BIN_Data_Encoder(df_train.index.values, df_train.Label.values, df_train)\n",
    "  training_generator = DataLoader(training_set, **params)\n",
    "  training_generator2 = DataLoader(training_set, **params2)\n",
    "\n",
    "  #phase1\n",
    "  print(\"-------phase1-------\")\n",
    "  model_max, loss_history = train(model,training_generator,epochs,batchsize,lr)\n",
    "  torch.save(model_max, 'model_max.pt') # 임시추가\n",
    "  loss1= pd.Series(loss_history)\n",
    "  loss1.to_csv('loss1.csv',index=False)\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  #phase2,3,4\n",
    "  print(\"-------phase2--------\")\n",
    "  training_generator_with_unlabel=Cus_DataLoader(training_generator, df_unlabel, model_max, device, confidence, ratio1, batchsize)\n",
    "  torch.cuda.empty_cache()\n",
    "  model_max2, loss_history2 = train(model2,training_generator_with_unlabel,epochs*2,batchsize*8,lr)\n",
    "  torch.save(model_max2, 'model_max2.pt') # 임시추가\n",
    "  loss2= pd.Series(loss_history2)\n",
    "  loss2.to_csv('loss2.csv',index=False)\n",
    "  torch.cuda.empty_cache()\n",
    "  # print(\"-------phase2 finetuning-------\")\n",
    "  # model_max2, loss_history2 = train(model_max2,training_generator2,epochs_fine*2,batchsize*4, lr)\n",
    "  # torch.cuda.empty_cache()\n",
    "\n",
    "  print(\"-------phase3--------\")\n",
    "  model_max2.batch_size=batchsize\n",
    "  training_generator_with_unlabel=Cus_DataLoader(training_generator, df_unlabel, model_max2, device, confidence, ratio1, batchsize)\n",
    "  torch.cuda.empty_cache()\n",
    "  model_max3, loss_history3 = train(model2,training_generator_with_unlabel,epochs*2,batchsize*8,lr)\n",
    "  torch.save(model_max3, 'model_max3.pt') # 임시추가\n",
    "  loss3= pd.Series(loss_history3)\n",
    "  loss3.to_csv('loss3.csv',index=False)\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  print(\"-------phase4--------\")\n",
    "  model_max3.batch_size=batchsize\n",
    "  training_generator_with_unlabel=Cus_DataLoader(training_generator, df_unlabel, model_max3, device, confidence, ratio2, batchsize)\n",
    "  torch.cuda.empty_cache()\n",
    "  model_max4, loss_history4 = train(model3,training_generator_with_unlabel,epochs*2,batchsize*16,lr)\n",
    "  torch.save(model_max4, 'model_max4.pt') # 임시추가\n",
    "  loss4= pd.Series(loss_history4)\n",
    "  loss4.to_csv('loss4.csv',index=False)\n",
    "  torch.cuda.empty_cache()\n",
    "  # print(\"-------phase3 finetuning-------\")\n",
    "  # model_max3, loss_history3 = train(model_max3,training_generator2,epochs_fine*3,batchsize*4,lr)\n",
    "  # torch.cuda.empty_cache()\n",
    "\n",
    "  print(\"-------phase5--------\")\n",
    "  model3 = BIN_Interaction_Flat3(**config)\n",
    "  model_comparison, loss_history_comparison = train(model3, training_generator2, epochs*2, batchsize*16, lr)\n",
    "  torch.save(model_comparison, 'model_comparison.pt') # 임시추가\n",
    "  loss_comparison= pd.Series(loss_history_comparison)\n",
    "  loss_comparison.to_csv('loss_comparison.csv',index=False)\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  print(\"-------training finished-------\")\n",
    "\n",
    "  return model_max, model_max2, model_max3, model_comparison, loss_history, loss_history2, loss_history3, loss_history_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AMfsnpIupjf",
    "outputId": "0bef5d4f-4ea0-4f65-c677-11b619442f81",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------phase1-------\n",
      "--- Data Preparation ---\n"
     ]
    }
   ],
   "source": [
    "s = time()\n",
    "model_max, model_max2, model_max3, model_comparison, loss_history, loss_history2, loss_history3, loss_history_comparison = main()\n",
    "e = time()\n",
    "print((e - s)/60)\n",
    "\n",
    "# torch.save(model_max, 'model_max.pt')\n",
    "# torch.save(model_max2, 'model_max2.pt')\n",
    "# torch.save(model_max3, 'model_max3.pt')\n",
    "# torch.save(model_comparison, 'model_comparison.pt')\n",
    "# loss= pd.Series(loss_history+loss_history2+loss_history3+loss_history_comparison)\n",
    "# loss.to_csv('loss.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzJy9ClbIkePlmeQdPtIhm",
   "machine_shape": "hm",
   "mount_file_id": "19OaXEA6ZJK6O35k7nv-PquQnJtx9_qhT",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": ".m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
